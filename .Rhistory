my_gsheet <- read.csv(my_gsheet$ws$exportcsv[1], stringsAsFactors = FALSE)
# split all the reponses by line breaks
make_tmp <- function(x){
tmp <- list()
for(i in seq_len(ncol(x))){
tmp[[i]] <- strsplit(as.character(x[,i]), "\n")
}
return(tmp)
}
tmp <- make_tmp(my_gsheet)
# get criteria and scores
library("rvest")
url <- "https://github.com/uwescience/reproducible/wiki/%5BDRAFT%5D-Open-Science-and-Reproducible-Badges"
scores <- url %>%
html() %>%
html_nodes(xpath = '//*[@id="wiki-body"]/div/table[1]') %>%
html_table() %>%
data.frame()
# get digits from the text
scores_digits <- as.numeric(gsub("[^0-9]", "", scores[,2]))
scores_each   <- grepl("each", scores[,2])
scores$scores_digits <- scores_digits
scores$scores_each <- scores_each
scores <- scores[!is.na(scores$scores_digits), ]
# clean up
scores <- scores[-22,]
scr_c <- function(person_, n){
scr_c <- 0
for(i in seq_along(person_)){
n <- length(unlist(person_[i]))
scr_c[i] <- unlist(ifelse(scores$scores_each[i], (n * scores[i, 3]),
ifelse(n == 0, 0, scores[i, 3])))
}
return(scr_c)
}
#
n <- 1
person <- lapply(tmp, function(i) unlist(i[n]))
person_ <- person[-c(1:2)]
scr_c <- scr_c(person_, n)
# score for each criterion
scr_df <- data.frame(criteria = scores$Criterion,
score = scr_c[1:24])
scr_df
scr_df <- data.frame(criteria = scores$Criterion,
score = scr_c[1:24])
sums <- sum(scr_c[1:24], na.rm = TRUE)
sums
ifelse(sums >= 50, "Gold",
ifelse(sums %in% 25:49, "Silver",   "Bronze"))
shiny::runApp()
scr_c[1:24]
src_c
scr_c_f <- function(person_, n){
scr_c <- 0
for(i in seq_along(person_)){
n <- length(unlist(person_[i]))
scr_c[i] <- unlist(ifelse(scores$scores_each[i], (n * scores[i, 3]),
ifelse(n == 0, 0, scores[i, 3])))
}
return(scr_c)
}
shiny::runApp()
shiny::runApp()
shiny::runApp()
shinyapps::deployApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shinyapps::deployApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shinyapps::deployApp()
shiny::runApp()
shinyapps::deployApp()
shiny::runApp()
links <- c("https://github.com/jheare/OluridaSurvey2014", "https://github.com/che625/olson-ms-nb", "https://github.com/sr320/eimd-sswd")
gsub('http\\S+\\s*',"", links)
grep('http\\S+\\s*', links)
grep('http.*', links)
shiny::runApp()
links <- c("https://github.com/jheare/OluridaSurvey2014", "https://github.com/che625/olson-ms-nb", "not a link", "https://github.com/sr320/eimd-sswd")
grepl('http.*', links)
links_idx <- grepl('http.*', links)
seq_along(links)
links[links_idx]
my_list <- c("https://github.com/jheare/OluridaSurvey2014", "https://github.com/che625/olson-ms-nb", "not a link", "https://github.com/sr320/eimd-sswd")
my_vec <- c("https://github.com/jheare/OluridaSurvey2014", "https://github.com/che625/olson-ms-nb", "not a link", "https://github.com/sr320/eimd-sswd")
links_idx <- grepl('http.*', my_vec)
my_vec[links_idx]
my_urls <- my_vec[links_idx]
for(i in seq_along(my_urls)){
paste0("<a href=",i,">",i,"</a>")
}
my_links <- list()
for(i in seq_along(my_urls)){
my_links[i] <- paste0("<a href=",i,">",i,"</a>")
}
my_links
my_links <- list()
for(i in seq_along(my_urls)){
my_links[i] <- paste0("<a href=",my_urls[i],">",my_urls[i],"</a>")
}
my_links
my_links <- list()
for(i in seq_along(my_urls)){
my_links[i] <- paste0("<a href='",my_urls[i],"'>",my_urls[i],"</a>")
}
my_links
my_links <- vector()
for(i in seq_along(my_urls)){
my_links[i] <- paste0("<a href='",my_urls[i],"'>",my_urls[i],"</a>")
}
my_links
n  <- 2
library(googlesheets)
library(dplyr)
my_gsheet_key <-  "1CrXOj26zohHvQ7RGzco4boqrXT7oNUeBZDPtFAyZdFQ"
my_gsheet <- gs_key(my_gsheet_key, visibility = "public")
# my_gsheet <- read.csv(my_gsheet$ws$exportcsv[1], stringsAsFactors = FALSE)
library(RCurl)
x <- getURL(my_gsheet$ws$exportcsv[1], .opts = list(ssl.verifypeer = FALSE))
my_gsheet <- read.csv(text = x, stringsAsFactors = FALSE)
# split all the reponses by line breaks
make_tmp <- function(x){
tmp <- list()
for(i in seq_len(ncol(x))){
tmp[[i]] <- strsplit(as.character(x[,i]), "\n")
}
return(tmp)
}
tmp <- make_tmp(my_gsheet)
# get criteria and scores
library("rvest")
url <- "https://github.com/uwescience/reproducible/wiki/%5BDRAFT%5D-Open-Science-and-Reproducible-Badges"
scores <- url %>%
html() %>%
html_nodes(xpath = '//*[@id="wiki-body"]/div/table[1]') %>%
html_table() %>%
data.frame()
# get digits from the text
scores_digits <- as.numeric(gsub("[^0-9]", "", scores[,2]))
scores_each   <- grepl("each", scores[,2])
scores$scores_digits <- scores_digits
scores$scores_each <- scores_each
scores <- scores[!is.na(scores$scores_digits), ]
# clean up
scores <- scores[-22,]
scr_c_f <- function(person_, n){
scr_c <- 0
for(i in seq_along(person_)){
n <- length(unlist(person_[i]))
scr_c[i] <- unlist(ifelse(scores$scores_each[i], (n * scores[i, 3]),
ifelse(n == 0, 0, scores[i, 3])))
}
return(scr_c)
}
#
person <- lapply(tmp, function(i) unlist(i[n]))
# make links clickable
sapply(person, function(i) paste0(i, collapse = ", "))[1:26]
responses <- sapply(person, function(i) paste0(i, collapse = ", "))[1:26]
links_idx <- grepl('http.*', responses)
links_idx
my_urls <- responses[links_idx]
my_urls
my_links <- vector()
for(i in seq_along(my_urls)){
my_links[i] <- paste0("<a href='",my_urls[i],"'>",my_urls[i],"</a>")
}
my_links
seq_along(responses)
responses
responses[1]
responses[2]
responses[3]
i  <- 3
links_idx <- grepl('http.*', responses[i])
links_idx
responses[4]
i  <- 4
links_idx <- grepl('http.*', responses[i])
links_idx
paste0(responses[i], collapse = ",")
strsplit(responses[i], split = ",| ")
unlist(strsplit(responses[i], split = ",| "))
tmp2[!tmp2 == ""]
tmp1[!tmp1 == ""]
tmp1 <- unlist(strsplit(responses[i], split = ",| "))
tmp1[!tmp1 == ""]
tmp3 <- grepl('http.*', tmp2)
tmp2 <- tmp1[!tmp1 == ""]
tmp3 <- grepl('http.*', tmp2)
tmp3
tmp3 <- tmp2[grepl('http.*', tmp2)]
tmp3
responses <- sapply(person, function(i) paste0(i, collapse = ", "))[1:26]
for(i in seq_along(responses)){
# string to vector
tmp1 <- unlist(strsplit(responses[i], split = ",| "))
# remove empty elements
tmp2 <- tmp1[!tmp1 == ""]
# get urls only, in case there are non-url entries
tmp3 <- tmp2[grepl('http.*', tmp2)]
# wrap with html
my_links <- vector()
for(i in seq_along(my_urls)){
my_links[i] <- paste0("<a href='",my_urls[i],"'>",my_urls[i],"</a>")
}
}
my_links
responses <- sapply(person, function(i) paste0(i, collapse = ", "))[1:26]
for(i in seq_along(responses)){
# string to vector
tmp1 <- unlist(strsplit(responses[i], split = ",| "))
# remove empty elements
tmp2 <- tmp1[!tmp1 == ""]
# get urls only, in case there are non-url entries
tmp3 <- tmp2[grepl('http.*', tmp2)]
# wrap with html
my_links <- vector()
for(i in seq_along(tmp3)){
my_links[i] <- paste0("<a href='",tmp3[i],"'>",tmp3[i],"</a>")
}
}
my_links
tmp3
responses
seq_along(responses)
unlist(strsplit(responses[i], split = ",| "))
i
i  <- 3
responses[i]
i  <- 4
unlist(strsplit(responses[i], split = ",| "))
tmp2 <- tmp1[!tmp1 == ""]
tmp2
tmp1 <- unlist(strsplit(responses[i], split = ",| "))
# remove empty elements
tmp2 <- tmp1[!tmp1 == ""]
tmp2
tmp1
tmp3 <- tmp2[grepl('http.*', tmp2)]
tmp3
my_links <- vector()
for(i in seq_along(tmp3)){
my_links[i] <- paste0("<a href='",tmp3[i],"'>",tmp3[i],"</a>")
}
my_links
responses[i] <- my_links
paste0(my_links, collapse = ", ")
responses[i] <- paste0(my_links, collapse = ", ")
responses <- sapply(person, function(i) paste0(i, collapse = ", "))[1:26]
for(i in seq_along(responses)){
# string to vector
tmp1 <- unlist(strsplit(responses[i], split = ",| "))
# remove empty elements
tmp2 <- tmp1[!tmp1 == ""]
# get urls only, in case there are non-url entries
tmp3 <- tmp2[grepl('http.*', tmp2)]
# wrap with html
my_links <- vector()
for(i in seq_along(tmp3)){
my_links[i] <- paste0("<a href='",tmp3[i],"'>",tmp3[i],"</a>")
}
responses[i] <- paste0(my_links, collapse = ", ")
}
responses
person <- lapply(tmp, function(i) unlist(i[n]))
# make links clickable
responses <- sapply(person, function(i) paste0(i, collapse = ", "))[1:26]
for(i in seq_along(responses)){
# string to vector
tmp1 <- unlist(strsplit(responses[i], split = ",| "))
# remove empty elements
tmp2 <- tmp1[!tmp1 == ""]
# get urls only, in case there are non-url entries
tmp3 <- tmp2[grepl('http.*', tmp2)]
# wrap with html
my_links <- vector()
for(i in seq_along(tmp3)){
my_links[i] <- paste0("<a href='",tmp3[i],"'>",tmp3[i],"</a>")
}
responses[i] <- paste0(my_links, collapse = ", ")
}
responses
i
person <- lapply(tmp, function(i) unlist(i[n]))
# make links clickable
responses <- sapply(person, function(i) paste0(i, collapse = ", "))[1:26]
for(i in seq_along(responses)){
# string to vector
tmp1 <- unlist(strsplit(responses[i], split = ",| "))
# remove empty elements
tmp2 <- tmp1[!tmp1 == ""]
# get urls only, in case there are non-url entries
tmp3 <- tmp2[grepl('http.*', tmp2)]
# wrap with html
my_links <- vector()
for(i in seq_along(tmp3)){
my_links[i] <- paste0("<a href='",tmp3[i],"'>",tmp3[i],"</a>")
}
responses[i] <- paste0(my_links, collapse = ", ")
print(i)
}
responses
unlist(strsplit(responses[i], split = ",| "))
i  <- 1
unlist(strsplit(responses[i], split = ",| "))
responses <- sapply(person, function(i) paste0(i, collapse = ", "))[1:26]
responses
i
unlist(strsplit(responses[i], split = ",| "))
tmp1 <- unlist(strsplit(responses[i], split = ",| "))
tmp1
tmp2 <- tmp1[!tmp1 == ""]
tmp2
tmp3 <- tmp2[grepl('http.*', tmp2)]
tmp3
my_links <- vector()
for(i in seq_along(tmp3)){
my_links[i] <- paste0("<a href='",tmp3[i],"'>",tmp3[i],"</a>")
}
my_links
responses[i]
responses[3]
responses[5]
responses[6]
str(responses[6])
length(responses[6])
grep(responses[i], "http")
grep(responses[6], "http")
grep(responses[4], "http")
grepl("http", responses[4])
grepl("http", responses[6])
person <- lapply(tmp, function(i) unlist(i[n]))
# make links clickable
responses <- sapply(person, function(i) paste0(i, collapse = ", "))[1:26]
for(i in seq_along(responses)){
# if there is http, do this loop, otherwise skip it
if(grepl("http", responses[i])){
# string to vector
tmp1 <- unlist(strsplit(responses[i], split = ",| "))
# remove empty elements
tmp2 <- tmp1[!tmp1 == ""]
# get urls only, in case there are non-url entries
tmp3 <- tmp2[grepl('http.*', tmp2)]
# wrap with html
my_links <- vector()
for(i in seq_along(tmp3)){
my_links[i] <- paste0("<a href='",tmp3[i],"'>",tmp3[i],"</a>")
}
responses[i] <- paste0(my_links, collapse = ", ")
print(i)
}
else
{
responses[i] <- responses[i]
}
}
responses
i  <- 4
grepl("http", responses[i]))
grepl("http", responses[i])
tmp1 <- unlist(strsplit(responses[i], split = ",| "))
tmp1
tmp2 <- tmp1[!tmp1 == ""]
tmp2
tmp3 <- tmp2[grepl('http.*', tmp2)]
tmp3
my_links <- vector()
for(i in seq_along(tmp3)){
my_links[i] <- paste0("<a href='",tmp3[i],"'>",tmp3[i],"</a>")
}
my_links
paste0(my_links, collapse = ", ")
responses[i] <- paste0(my_links, collapse = ", ")
print(i)
seq_along(responses)
n
person <- lapply(tmp, function(i) unlist(i[n]))
# make links clickable
responses <- sapply(person, function(i) paste0(i, collapse = ", "))[1:26]
for(i in seq_along(responses)){
# if there is http, do this loop, otherwise skip it
if(grepl("http", responses[i])){
# string to vector
tmp1 <- unlist(strsplit(responses[i], split = ",| "))
# remove empty elements
tmp2 <- tmp1[!tmp1 == ""]
# get urls only, in case there are non-url entries
tmp3 <- tmp2[grepl('http.*', tmp2)]
# wrap with html
my_links <- vector()
for(i in seq_along(tmp3)){
my_links[i] <- paste0("<a href='",tmp3[i],"'>",tmp3[i],"</a>")
}
responses[i] <- paste0(my_links, collapse = ", ")
print(i)
}
else
{
responses[i] <- responses[i]
print(i)
}
}
responses
i  <- 8
grepl("http", responses[i])
tmp1 <- unlist(strsplit(responses[i], split = ",| "))
# remove empty elements
# get urls only, in case there are non-url entries
tmp2 <- tmp1[!tmp1 == ""]
tmp3 <- tmp2[grepl('http.*', tmp2)]
# wrap with html
my_links <- vector()
for(i in seq_along(tmp3)){
my_links[i] <- paste0("<a href='",tmp3[i],"'>",tmp3[i],"</a>")
}
responses[i] <- paste0(my_links, collapse = ", ")
print(i)
person <- lapply(tmp, function(i) unlist(i[n]))
# make links clickable
responses <- sapply(person, function(i) paste0(i, collapse = ", "))[1:26]
for(i in seq_along(responses)){
# if there is http, do this loop, otherwise skip it
if(grepl("http", responses[i])){
# string to vector
tmp1 <- unlist(strsplit(responses[i], split = ",| "))
# remove empty elements
tmp2 <- tmp1[!tmp1 == ""]
# get urls only, in case there are non-url entries
tmp3 <- tmp2[grepl('http.*', tmp2)]
# wrap with html
my_links <- vector()
for(j in seq_along(tmp3)){
my_links[j] <- paste0("<a href='",tmp3[j],"'>",tmp3[j],"</a>")
}
responses[i] <- paste0(my_links, collapse = ", ")
print(i)
}
else
{
responses[i] <- responses[i]
print(i)
}
}
responses
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
library(googlesheets)
library(dplyr)
my_gsheet_key <-  "1CrXOj26zohHvQ7RGzco4boqrXT7oNUeBZDPtFAyZdFQ"
my_gsheet <- gs_key(my_gsheet_key, visibility = "public")
# my_gsheet <- read.csv(my_gsheet$ws$exportcsv[1], stringsAsFactors = FALSE)
library(RCurl)
x <- getURL(my_gsheet$ws$exportcsv[1], .opts = list(ssl.verifypeer = FALSE))
my_gsheet <- read.csv(text = x, stringsAsFactors = FALSE)
my_gsheet
n
person <- lapply(tmp, function(i) unlist(i[n]))
# make links clickable
responses <- sapply(person, function(i) paste0(i, collapse = ", "))[1:26]
responses
n  <- 1
person <- lapply(tmp, function(i) unlist(i[n]))
# make links clickable
responses <- sapply(person, function(i) paste0(i, collapse = ", "))[1:26]
responses
for(i in seq_along(responses)){
# if there is http, do this loop, otherwise skip it
if(grepl("http", responses[i])){
# string to vector
tmp1 <- unlist(strsplit(responses[i], split = ", | "))
# remove empty elements
tmp2 <- tmp1[!tmp1 == ""]
# get urls only, in case there are non-url entries
tmp3 <- tmp2[grepl('http.*', tmp2)]
# wrap with html
my_links <- vector()
for(j in seq_along(tmp3)){
my_links[j] <- paste0("<a href='",tmp3[j],"'>",tmp3[j],"</a>")
}
responses[i] <- paste0(my_links, collapse = ", ")
print(i)
}
else
# what to do if the row has no http in it...
{
responses[i] <- responses[i]
print(i)
}
}
responses
shiny::runApp()
library(shinyapps)
shinyapps::deployApp()
devtools::install_github("benmarwick/JSTORr")
install.packages("XML")
install.packages("XML")
devtools::install_github("benmarwick/JSTORr")
library(JSTORr)
?JSTOR_subset1grams
JSTOR_subset1grams
